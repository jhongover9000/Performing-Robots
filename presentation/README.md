### On Hyper-Realistic Facial Expressions

(SCRIPT BEGIN)

Ameca, the robot with the most human-like expressions in robotics, and how 
this can evoke emotions in performance. So what exactly is Ameca?

Ameca is a humanoid robot created by Engineered Arts to display 
hyper-realistic human facial expressions. It caused quite a stir on the 
Internet, a lot of people thinking it was CGI. But it wasn’t. Pretty cool, 
huh?

On a side note, Ameca is coming to Dubai in the Museum of the Future… Just 
saying. Just saying. *cough* FIELD TRIP *cough*

Anyway, the relationship between emotions and expressions are rather 
complex. In general, there are seven main categories of emotional 
expression that can be found in more or less all cultures. These include 
anger, contempt, disgust, fear, happiness, sadness, and surprise–– oh hey, 
that’s Inside Out! Anyway, even if there are some differences between 
cultures on the expressions themselves, the way they’re conveyed is done 
with what I wanna call the emotion-expression-emotion system.
You start with an emotion. You express that emotion. Upon seeing the 
expression the other party feels the emotion that you have or are trying 
to convey. Sure, it’s not always a 100% hit and there are a lot of edge 
cases, but they get most of our message and it’s generally what we’re 
feeling.

Now, what exactly does this have to do with robots and their expressions? 
Well, what we’re basically pulling off here with making hyperrealistic 
human facial expressions, or just expression in general, is (slide change) 
we’re getting rid of the initial emotion and moving straight to the 
expression and conveying of emotion. Does it work? It’s just a question to 
keep in mind.

Using 17 motors and a mix of fine fibers across the skin, Ameca is capable 
of producing very fine expressions using its eyebrows and mouth. The 
smiling itself took over ten iterations to complete, particularly because 
of the creases here *indicate*, and much more can be said for the rest of 
its expressions. The result, though, is a pretty cool robot capable of 
making a variety of pretty convincing expressions.

Now, how can we take that a step further? How can you convey not only 
emotion but maybe even a story through a robot-human interaction? Well, 
one answer is the (slide change) reaction of the robot to outside stimuli, something that we talked about in our discussions. It splits into the 
speed and logic of the reaction, the dynamic range of the response. When 
it comes to human-like robots, though, maybe there’s another factor to 
it–– how does the robot “interpret” the stimulus, and how does it show on 
its face?

Ameca is programmed to be able to respond to different stimuli using a 
variety of its sensors and motors. The engineers spent lots and lots of 
time trying to understand how exactly humans move and respond to stimuli, 
and it shows in the human-like response that it gives.

An engineer? Interacting with *gasp* humans? To create a robot, though, so 
the sacrifice must’ve been deemed necessary.

However, Ameca is merely a robot interface, meaning that it doesn’t have 
an artificial intelligence system working in it like Sophia. It’s possible 
to add different models, such as the GPT-3 speech model, to it, but in 
general Ameca will be making do with whatever it’s given.

In that sense, the expression-emotion system from earlier may be a bit too 
much of an abstraction. Maybe we’re missing a step. Something like (slide 
change) program? Or (slide change) process? (slide change) Something like 
that, anyway.

Looping back to a Oliver’s presentation a while back, let’s talk uncanny. 
How’d Ameca seem to you? A bit less uncanny and more human, hopefully. At 
least in terms of movement and fluidity.

The gray skin, translucent head, and wired body? You see, it’s actually 
intentional, meant to pull Ameca backwards out of the uncanny valley. The 
point of Ameca wasn’t to create photorealistic robots but rather to focus 
capturing the biomechanics of humans in machines. In other words, the 
creators knew that it was going to be harder to make Ameca seem human 
because of the additional influences that would occur if they decided to 
make it look even more human–– viewers would unconsciously pick up more 
visual cues and perhaps even be more critical of its fluidity.

And maybe that’s where the split is. The creators called Ameca a “learning 
experience,” as it helped them understand just how far we are from 
fully-functioning and fluid human-like androids. Even with its 
expressions, there’s a limit. The human face has 43 different muscles 
capable of producing microexpressions that last for 1/25th of a second. 
Ameca only has 17 motors to produce these movements, and natural 
microexpressions are a long way off.

Humans are so profoundly complex in how dense, powerful, and fluid our 
muscles are. Creating a neck that has fluid movement while retaining a 
smaller size took many prototypes, for example. And getting closer to a 
human form apparently makes robots have extreme difficulty walking, the 
act of constantly being off-balance being one of the larger reasons.
In any case, even though it seems like we are getting close to the 
Singularity, there’s actually a lot more to do and learn, particularly in 
the realm of movement.

But let’s put that aside for a sec. What if, what if, we could get there, 
to where we can get these robots to produce not only hyperrealistic facial 
expressions but also bodily reactions and movements? What might that mean, 
particularly in the field of performance?

If expressions can, in fact, convey emotion, and robots do need only seem 
like they’re showing emotion, this could be an interesting development in 
the areas of performance.

The technology to recreate actors’ faces and follow their expressions 
exists, and are being used frequently in the movie industry. 

CGI for hyperrealistic faces and even digital doubles are also advancing; 
the release of Unreal Engine 5 only adds to this.

Could this “humanness” extend into the realm of physical performance, of 
performing robots, in the fields of theater and musical? And how might we 
view these performances–– as robots, or as something more?

If robots can look human, sound human, and even evoke our emotions the 
same way a human can through their gestures and facial expressions, will 
the line between human and robot performers thin to the point where it 
becomes negligible?

But it wouldn’t stop there, would it? You see, Ameca and a lot of these 
robots are designed not exactly for the performance sector–– though they 
would definitely be capable of this ––but rather for the service sector. 

If robots are capable of “displaying” emotions and empathy, even if they 
themselves aren’t feeling what they express, the potential for robots in 
the service sector are endless. Think education and healthcare, daycare 
and nursing homes. Think information desks, think taxi drivers, think 
flight attendants. Think every part of the service sector that we know of, 
and think of how that might begin to seep into more than just that.

It won’t be a forceful takeover, like we get afraid of sometimes. We 
ourselves would be replacing us with robots; we would be creating our own 
dependency. But we wouldn’t mind. We would be content where we are. In a 
world run by the works of our own hands, a world run by robots.

But that’s not gonna happen, right? Because we’re responsible, right? 
Because we’ll stop before things get out of hand, right?

And it’s a long time away, so we won’t need to worry about it, right? Or.. 
is it..?

I suppose only time will tell.

(SCRIPT END)
