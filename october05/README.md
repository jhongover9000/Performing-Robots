### The Freudian Robot - A Reading Response

What exactly *is* a Freudian robot? It feels like the paper never explicitly answers the question aside from the needlessly ambiguous and wordy answer that "any networked being that embodies the feedback loop of human-machine simulacra and cannot free her/him/itself from the cybernetic unconscious is a Freudian robot". Like, okay, what is that supposed to mean, exactly? We're all Freudian robots, I guess..?

There's a lot of talk about the feedback loop, as well as the interaction between robots and humans. It's true, there's been a lot of development and the way that we approach machines and artificial intelligence has changed drastically. We are able to do much more than before with the same kind of simple logic and bits from over half a century ago. The feedback loop is visible in just about very CS student when they try to take apart an issue like it's a coding problem (lol). But seriously, a lot of thinking becomes restrained, in a way, by the logic that exists in low-level programming. A lot can be done, but not everything. That might change with quantum computing, thouh, so that's always really exciting :)

We're talking a lot about *artificial intelligence* here. At this point we keep talking about how they *think* or *act* and how that's similar to humans, but this is largely software-based. Of course, designing it in a certain way will restrain its actions to the shape it is given, but the artificially intelligent mind in the end is a sum of systematic thinking that is programmed into it.

The question in itself, the feedback cycle itself, seems pointless to me. Humans created the computational mind *based* on human thought and logic; it was to simulate how humans think on a simpler scale. And then, using that simple logic, we've been trying to do more complex things. Think of sudoku solvers or state-machines, which go from one state to the next in order to get closer to a goal state using heuristics or whatnot. In other words, it is a code written by humans that uses data collected or given to it by humans. And the shift towards machine learning is something similar, an attempt to simulate/emulate the brain and its neurons.

But have we become dependent on robots, on artificial intelligence? I would say so. There's a lot of things that we don't realize run on artificial intelligence, as well as things that we simply cannot go back to now that we've digitalized it. Hospital records are some of these things, so crucial to the point that many hospitals refrain from updating their systems because they are so reliant on a specific version and protocol.

Will there come a time when humans will decide to treat robots in a similar moral way we treat other humans? Perhaps, but I don't think that'll happen until we a) find a way to generate true randomness, b) understand (and I mean *understand*) how the human brain works, c) create algorithms or models that can emulate that behavior, among other things. But do robots need to be sentient in order for us to decide that they should be treated with some kind of morality? That's also not really something that should be decided at this point.

It feels like we're talking too far into the future and into hypotheticals. Right now, artificial intelligence can only work with what it's been given. The program reflects its programmer. But in the presence of a computing power far greater than our own (using simple logic that can be compounded into something greater), we have grown very dependent on this type of intelligence, on the robots that we've created.
